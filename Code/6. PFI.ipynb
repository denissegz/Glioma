{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36f09d39-93e9-4aad-be71-3a44f9598a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sksurv.meta import EnsembleSelection\n",
    "from sksurv.svm import FastSurvivalSVM, FastKernelSurvivalSVM\n",
    "from sksurv.tree import SurvivalTree\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis, CoxnetSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, StratifiedKFold, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif, VarianceThreshold, RFECV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.ensemble import GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn import set_config\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import pickle\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from functools import cache\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "randomState = 42 # tip of the cap to Douglas Adams\n",
    "set_config(transform_output=\"pandas\")\n",
    "Verbosity = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b6e52b7-ffaa-42c1-8bd4-a516afa1dc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fianl_clean_data = pd.read_pickle('fianl_clean_data.pkl')\n",
    "fianl_clean_data = pd.read_pickle(\"C:\\\\Users\\\\T00758722\\\\Downloads\\\\fianl_clean_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2cc0940-df9a-4598-a9e9-0ef31ad662db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cg13869341</th>\n",
       "      <th>cg24669183</th>\n",
       "      <th>cg15560884</th>\n",
       "      <th>cg01014490</th>\n",
       "      <th>cg17505339</th>\n",
       "      <th>cg11954957</th>\n",
       "      <th>cg16736630</th>\n",
       "      <th>cg05898754</th>\n",
       "      <th>cg03128332</th>\n",
       "      <th>cg16619049</th>\n",
       "      <th>...</th>\n",
       "      <th>gender</th>\n",
       "      <th>initial_pathologic_dx_year</th>\n",
       "      <th>birth_days_to</th>\n",
       "      <th>OS</th>\n",
       "      <th>OS.time</th>\n",
       "      <th>DSS</th>\n",
       "      <th>DSS.time</th>\n",
       "      <th>PFI</th>\n",
       "      <th>PFI.time</th>\n",
       "      <th>classification.2021_simplified.labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>369.0</th>\n",
       "      <td>0.839668</td>\n",
       "      <td>0.622626</td>\n",
       "      <td>0.724557</td>\n",
       "      <td>0.013253</td>\n",
       "      <td>0.954777</td>\n",
       "      <td>0.778680</td>\n",
       "      <td>0.937495</td>\n",
       "      <td>0.574408</td>\n",
       "      <td>0.256935</td>\n",
       "      <td>0.346032</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>-22392.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96.0</th>\n",
       "      <td>0.887214</td>\n",
       "      <td>0.557666</td>\n",
       "      <td>0.709998</td>\n",
       "      <td>0.016885</td>\n",
       "      <td>0.909813</td>\n",
       "      <td>0.859712</td>\n",
       "      <td>0.885200</td>\n",
       "      <td>0.341588</td>\n",
       "      <td>0.088496</td>\n",
       "      <td>0.115528</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>-23343.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1448.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1448.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>797.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423.0</th>\n",
       "      <td>0.878038</td>\n",
       "      <td>0.821218</td>\n",
       "      <td>0.665864</td>\n",
       "      <td>0.017920</td>\n",
       "      <td>0.925042</td>\n",
       "      <td>0.815867</td>\n",
       "      <td>0.818661</td>\n",
       "      <td>0.365594</td>\n",
       "      <td>0.115413</td>\n",
       "      <td>0.206443</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>-18659.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370.0</th>\n",
       "      <td>0.857886</td>\n",
       "      <td>0.765783</td>\n",
       "      <td>0.677472</td>\n",
       "      <td>0.008228</td>\n",
       "      <td>0.952425</td>\n",
       "      <td>0.691414</td>\n",
       "      <td>0.915337</td>\n",
       "      <td>0.337019</td>\n",
       "      <td>0.258516</td>\n",
       "      <td>0.078493</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>-19237.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459.0</th>\n",
       "      <td>0.851489</td>\n",
       "      <td>0.727787</td>\n",
       "      <td>0.790580</td>\n",
       "      <td>0.011233</td>\n",
       "      <td>0.909230</td>\n",
       "      <td>0.632709</td>\n",
       "      <td>0.839941</td>\n",
       "      <td>0.651802</td>\n",
       "      <td>0.071423</td>\n",
       "      <td>0.607906</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>-15950.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>953.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>953.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>953.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 403966 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cg13869341  cg24669183  cg15560884  cg01014490  cg17505339  cg11954957  \\\n",
       "369.0    0.839668    0.622626    0.724557    0.013253    0.954777    0.778680   \n",
       "96.0     0.887214    0.557666    0.709998    0.016885    0.909813    0.859712   \n",
       "423.0    0.878038    0.821218    0.665864    0.017920    0.925042    0.815867   \n",
       "370.0    0.857886    0.765783    0.677472    0.008228    0.952425    0.691414   \n",
       "459.0    0.851489    0.727787    0.790580    0.011233    0.909230    0.632709   \n",
       "\n",
       "       cg16736630  cg05898754  cg03128332  cg16619049  ...  gender  \\\n",
       "369.0    0.937495    0.574408    0.256935    0.346032  ...       1   \n",
       "96.0     0.885200    0.341588    0.088496    0.115528  ...       0   \n",
       "423.0    0.818661    0.365594    0.115413    0.206443  ...       1   \n",
       "370.0    0.915337    0.337019    0.258516    0.078493  ...       1   \n",
       "459.0    0.839941    0.651802    0.071423    0.607906  ...       1   \n",
       "\n",
       "       initial_pathologic_dx_year  birth_days_to   OS  OS.time  DSS  DSS.time  \\\n",
       "369.0                      2004.0       -22392.0  1.0     24.0  1.0      24.0   \n",
       "96.0                       2001.0       -23343.0  1.0   1448.0  1.0    1448.0   \n",
       "423.0                      2008.0       -18659.0  1.0    141.0  1.0     141.0   \n",
       "370.0                      2003.0       -19237.0  1.0     42.0  1.0      42.0   \n",
       "459.0                      2008.0       -15950.0  0.0    953.0  0.0     953.0   \n",
       "\n",
       "       PFI  PFI.time  classification.2021_simplified.labels  \n",
       "369.0  1.0      24.0                                      1  \n",
       "96.0   1.0     797.0                                      1  \n",
       "423.0  1.0      81.0                                      1  \n",
       "370.0  1.0      42.0                                      1  \n",
       "459.0  0.0     953.0                                      0  \n",
       "\n",
       "[5 rows x 403966 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fianl_clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "380eb443-304a-47d6-9a10-369e07a15352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fianl_clean_data[['PFI']].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e017285-094e-477e-bea1-3d0db5ba27a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# break out the target\n",
    "# print(mergeSurvivalClinicalGene.describe())\n",
    "y = fianl_clean_data[['PFI']]\n",
    "# y['PFIBool'] = y.PFI.astype(bool) # this is the format scikit-survival needs\n",
    "fianl_clean_data.drop(columns=['PFI'], inplace=True)\n",
    "# mergeSurvivalClinicalGene.describe()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff96cf2c-1d44-423e-ae0d-1729d2dafaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(fianl_clean_data, y, test_size=0.2, random_state=randomState, stratify=y)\n",
    "\n",
    "if Verbosity > 0:\n",
    "    print('X_train', X_train.shape), print('y_train', y_train.shape)\n",
    "    # print(np.bincount(y_train))\n",
    "    print('X_test', X_test.shape), print('y_test', y_test.shape)\n",
    "    # print(np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07192102-43eb-4f34-ad0d-4fcde95a4fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we've split and stratified, supplement y with other elements of target\n",
    "y['PFI.time'] = fianl_clean_data['PFI.time']\n",
    "y_train['PFI.time'] = X_train['PFI.time']\n",
    "y_test['PFI.time'] = X_test['PFI.time']\n",
    "fianl_clean_data.drop(columns='PFI.time', inplace=True)\n",
    "X_train.drop(columns='PFI.time', inplace=True)\n",
    "X_test.drop(columns='PFI.time', inplace=True)\n",
    "# yPFI_train = [[y_train, X_train['PFI.time']]\n",
    "# yPFI_train = [[y_test, X_test['PFI.time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a8a0f12-89b7-4ea7-8fa0-f58865cd1fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create boolean target that scikit-survival needs\n",
    "y['PFIBool'] = y.PFI.astype(bool)\n",
    "y_train['PFIBool'] = y_train.PFI.astype(bool)\n",
    "y_test['PFIBool'] = y_test.PFI.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6026cf2f-961a-40e3-ab25-8c8614b5ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert PFI.time from days to months\n",
    "# this makes charts easier to interpret\n",
    "y['PFI.time.months'] = y['PFI.time'] / 30.417\n",
    "\n",
    "if Verbosity > 0:\n",
    "    y['PFI.time.months']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abae0163-ca47-499b-8ac3-499cc3742f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Kaplan-Meier Survival Function for the entire dataset\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "\n",
    "surv_time, survival_prob, conf_int = kaplan_meier_estimator(\n",
    "    y.PFIBool, y['PFI.time.months'], conf_type=\"log-log\"\n",
    "    # data_y[\"Status\"], data_y[\"Survival_in_days\"], conf_type=\"log-log\"\n",
    ")\n",
    "plt.step(surv_time, survival_prob, where=\"post\")\n",
    "plt.fill_between(surv_time, conf_int[0], conf_int[1], alpha=0.25, step=\"post\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Kaplan-Meier Survival Function')\n",
    "plt.ylabel(\"est. probability of survival $\\hat{S}(t)$\")\n",
    "plt.xlabel(\"time $t$ (months)\")\n",
    "plt.savefig(\"Kaplan-Meier Survival Function.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2d4cde3-77e8-46c4-9da0-eed1f29e1297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to format target for Scikit-Survival\n",
    "#\n",
    "def format_target_for_Scikit_Survival(y):\n",
    "    # y: event boolean and event time, eg. y_train[['PFIBool', 'PFI.time']]\n",
    "    # List of tuples\n",
    "    aux = [(e1,e2) for e1,e2 in np.array(y)]\n",
    "    # aux = [(e1,e2) for e1,e2 in np.array(y[['PFIBool', 'PFI.time']])]\n",
    "\n",
    "    #Structured array\n",
    "    yPFI = np.array(aux, dtype=[('Status', '?'), ('Survival_in_months', '<f8')])\n",
    "    # print(yDFI)\n",
    "\n",
    "    return yPFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "722b8bf6-98ec-424a-86ea-3615d05af66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format target for Scikit-Survival\n",
    "\n",
    "#Structured array\n",
    "yPFI = format_target_for_Scikit_Survival(y[['PFIBool', 'PFI.time']])\n",
    "if Verbosity > 0:\n",
    "    yPFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b26524d4-2cba-421c-82bb-9f0a6e16e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format target for Scikit-Survival\n",
    "yPFI_train = format_target_for_Scikit_Survival(y_train[['PFIBool', 'PFI.time']])\n",
    "if Verbosity > 0:\n",
    "    yPFI_train\n",
    "\n",
    "yPFI_test = format_target_for_Scikit_Survival(y_test[['PFIBool', 'PFI.time']])\n",
    "if Verbosity > 0:\n",
    "    yPFI_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aaf9ce02-0ec7-4e0a-9b42-793322df2344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create additional target at 'years' years in to the study\n",
    "def create_target_year(X, y, years):\n",
    "    PFIBinary = []\n",
    "    PFITimeMonths = []\n",
    "\n",
    "    for sample_idx in range(len(y)):\n",
    "        # print(y.iloc[sample_idx])\n",
    "        if y.iloc[sample_idx]['PFI.time.months'] <= (years * 12):\n",
    "            # print('In Cohort, DFI= ', y.iloc[sample_idx].PFI)\n",
    "            PFIBinary.append(y.iloc[sample_idx].PFI)\n",
    "            PFITimeMonths.append(y.iloc[sample_idx]['PFI.time.months'])\n",
    "        else:\n",
    "            PFIBinary.append(0) # censored\n",
    "            PFITimeMonths.append(years * 12)\n",
    "\n",
    "    PFIBinary = pd.Series(PFIBinary)\n",
    "    PFIBool = PFIBinary.astype(bool)\n",
    "    PFITimeMonths = pd.Series(PFITimeMonths)\n",
    "\n",
    "    # create balanced test and train sets\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, PFIBinary, test_size=0.2, random_state=randomState, stratify=PFIBinary)\n",
    "\n",
    "    if Verbosity > 0:\n",
    "        print('ytrain: ', np.bincount(ytrain))\n",
    "        print('ytrain: ', ytrain)\n",
    "        print('ytest: ', np.bincount(ytest))\n",
    "        print('ytrain: ', ytrain)\n",
    "\n",
    "    return Xtrain, Xtest, ytrain, ytest, PFIBinary, PFIBool, PFITimeMonths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7f40415-a380-4ec2-967d-8bab9b66826a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PFI</th>\n",
       "      <td>578.0</td>\n",
       "      <td>0.472318</td>\n",
       "      <td>0.499666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PFI.time</th>\n",
       "      <td>578.0</td>\n",
       "      <td>651.467128</td>\n",
       "      <td>695.607287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>194.250000</td>\n",
       "      <td>455.00000</td>\n",
       "      <td>857.500000</td>\n",
       "      <td>5255.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PFI.time.months</th>\n",
       "      <td>578.0</td>\n",
       "      <td>21.417863</td>\n",
       "      <td>22.869030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.386231</td>\n",
       "      <td>14.95874</td>\n",
       "      <td>28.191472</td>\n",
       "      <td>172.76523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 count        mean         std  min         25%        50%  \\\n",
       "PFI              578.0    0.472318    0.499666  0.0    0.000000    0.00000   \n",
       "PFI.time         578.0  651.467128  695.607287  0.0  194.250000  455.00000   \n",
       "PFI.time.months  578.0   21.417863   22.869030  0.0    6.386231   14.95874   \n",
       "\n",
       "                        75%         max  \n",
       "PFI                1.000000     1.00000  \n",
       "PFI.time         857.500000  5255.00000  \n",
       "PFI.time.months   28.191472   172.76523  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f4c3d57-056e-4cbb-881f-67d79d8ce182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 1, 3, 5 year targets\n",
    "\n",
    "y_timeframes = []\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest, yPFIBinary, yPFIBool, yPFITimeMonths = create_target_year(fianl_clean_data, y, 1)\n",
    "y_timeframes += [['Year1', Xtrain, Xtest, ytrain, ytest, yPFIBinary, yPFIBool, yPFITimeMonths]]\n",
    "# y_timeframes += [['Year1', y['PFIYear1'], y['PFIYear1.time.months'], y['PFIYear1Bool']]]\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest, yPFIBinary, yPFIBool, yPFITimeMonths = create_target_year(fianl_clean_data, y, 3)\n",
    "y_timeframes += [['Year3', Xtrain, Xtest, ytrain, ytest, yPFIBinary, yPFIBool, yPFITimeMonths]]\n",
    "# y['PFIYear3'], y['PFIYear3Bool'], y['PFIYear3.time.months'] = create_target_year(y, 3)\n",
    "#y_timeframes += [['Year3', y['PFIYear3'], y['PFIYear3.time.months'], y['PFIYear3Bool']]]\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest, yPFIBinary, yPFIBool, yPFITimeMonths = create_target_year(fianl_clean_data, y, 5)\n",
    "y_timeframes += [['Year5', Xtrain, Xtest, ytrain, ytest, yPFIBinary, yPFIBool, yPFITimeMonths]]\n",
    "# y['PFIYear5'], y['PFIYear5Bool'], y['PFIYear5.time.months'] = create_target_year(y, 5)\n",
    "# y_timeframes += [['Year5', y['PFIYear5'], y['PFIYear5.time.months'], y['PFIYear5Bool']]]\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest, yPFIBinary, yPFIBool, yPFITimeMonths = create_target_year(fianl_clean_data, y, 7)\n",
    "y_timeframes += [['Year7', Xtrain, Xtest, ytrain, ytest, yPFIBinary, yPFIBool, yPFITimeMonths]]\n",
    "\n",
    "y_timeframes += [['End Of Study', X_train, X_test, y_train.PFI, y_test.PFI, y.PFI, y.PFIBool, y['PFI.time.months']]]\n",
    "# print('ytrain: ', np.bincount(y_train))\n",
    "# print('ytest: ', np.bincount(y_test))\n",
    "\n",
    "# y_timeframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6136cdb0-5ed6-47ac-8792-53ec79bdbd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeframe: Year1\n",
      "DFI.time.months: count    578.000000\n",
      "mean       9.322400\n",
      "std        3.887837\n",
      "min        0.000000\n",
      "25%        6.386231\n",
      "50%       12.000000\n",
      "75%       12.000000\n",
      "max       12.000000\n",
      "dtype: float64\n",
      "==================================================\n",
      "Timeframe: Year3\n",
      "DFI.time.months: count    578.000000\n",
      "mean      17.263340\n",
      "std       12.122411\n",
      "min        0.000000\n",
      "25%        6.386231\n",
      "50%       14.958740\n",
      "75%       28.191472\n",
      "max       36.000000\n",
      "dtype: float64\n",
      "==================================================\n",
      "Timeframe: Year5\n",
      "DFI.time.months: count    578.000000\n",
      "mean      19.544799\n",
      "std       16.454539\n",
      "min        0.000000\n",
      "25%        6.386231\n",
      "50%       14.958740\n",
      "75%       28.191472\n",
      "max       60.000000\n",
      "dtype: float64\n",
      "==================================================\n",
      "Timeframe: Year7\n",
      "DFI.time.months: count    578.000000\n",
      "mean      20.557191\n",
      "std       19.335580\n",
      "min        0.000000\n",
      "25%        6.386231\n",
      "50%       14.958740\n",
      "75%       28.191472\n",
      "max       84.000000\n",
      "dtype: float64\n",
      "==================================================\n",
      "Timeframe: End Of Study\n",
      "DFI.time.months: count    578.000000\n",
      "mean      21.417863\n",
      "std       22.869030\n",
      "min        0.000000\n",
      "25%        6.386231\n",
      "50%       14.958740\n",
      "75%       28.191472\n",
      "max      172.765230\n",
      "Name: PFI.time.months, dtype: float64\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each time frame in y_timeframes\n",
    "for timeframe_name, Xtrain, Xtest, ytrain, ytest, yPFIBinary, yPFIBool, yPFITimeMonths in y_timeframes:\n",
    "    print(f\"Timeframe: {timeframe_name}\")\n",
    "    print(f\"DFI.time.months: {yPFITimeMonths.describe()}\")\n",
    "    print(\"=\" * 50)  # Just to separate each timeframe with a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb20f161-c946-4180-abeb-8e241f4106f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for Year1:\n",
      "  Training data shape: (462, 403964)\n",
      "  Testing data shape: (116, 403964)\n",
      "  Training target distribution: [345 117]\n",
      "  Testing target distribution: [86 30]\n",
      "Data for Year3:\n",
      "  Training data shape: (462, 403964)\n",
      "  Testing data shape: (116, 403964)\n",
      "  Training target distribution: [271 191]\n",
      "  Testing target distribution: [68 48]\n",
      "Data for Year5:\n",
      "  Training data shape: (462, 403964)\n",
      "  Testing data shape: (116, 403964)\n",
      "  Training target distribution: [254 208]\n",
      "  Testing target distribution: [64 52]\n",
      "Data for Year7:\n",
      "  Training data shape: (462, 403964)\n",
      "  Testing data shape: (116, 403964)\n",
      "  Training target distribution: [249 213]\n",
      "  Testing target distribution: [63 53]\n",
      "Data for End of Study:\n",
      "  Training data shape: (462, 403964)\n",
      "  Testing data shape: (116, 403964)\n",
      "  Training target distribution: [244 218]\n",
      "  Testing target distribution: [61 55]\n"
     ]
    }
   ],
   "source": [
    "for name, frame in zip([\"Year1\", \"Year3\", \"Year5\", \"Year7\", \"End of Study\"], y_timeframes):\n",
    "    print(f\"Data for {name}:\")\n",
    "    _, Xtrain, Xtest, ytrain, ytest, _, _, _ = frame\n",
    "    print(\"  Training data shape:\", Xtrain.shape)\n",
    "    print(\"  Testing data shape:\", Xtest.shape)\n",
    "    print(\"  Training target distribution:\", np.bincount(ytrain))\n",
    "    print(\"  Testing target distribution:\", np.bincount(ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cef19d3d-efb4-434a-9116-57cd374677d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility pipeline debugging class\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class Debug(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X):\n",
    "        if Verbosity > 0:\n",
    "            print('--- Debug transform -------------------------------------------------------------\\n')\n",
    "            print('X: ', X.shape)\n",
    "            print('X: \\n', pd.DataFrame(X).head())\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        if Verbosity > 0:\n",
    "            print('--- Debug fit -------------------------------------------------------------\\n')\n",
    "            print('X: ', X.shape)\n",
    "            print('X: \\n', pd.DataFrame(X).head())\n",
    "            with open('X.pkl', 'wb') as f:\n",
    "                pickle.dump(X, f)\n",
    "            print('y: ', y.shape)\n",
    "            print('y: \\n', pd.DataFrame(y).head())\n",
    "            with open('y.pkl', 'wb') as f:\n",
    "                pickle.dump(y, f)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99efe155-e3a5-4746-afb5-a2d0c1edf528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with dtype 'object': []\n"
     ]
    }
   ],
   "source": [
    "# Assuming clinical_info is a pandas DataFrame already loaded\n",
    "\n",
    "# Select columns where dtype is 'object'\n",
    "object_columns = fianl_clean_data.select_dtypes(include=['object']).columns\n",
    "nomFeatureColNames=[]\n",
    "# Convert the Index object to a list\n",
    "nomFeatureColNames = object_columns.tolist()\n",
    "\n",
    "# Print the list of column names\n",
    "# nomFeatureColNames.remove('Unnamed: 0')\n",
    "# nomFeatureColNames.remove('ID')\n",
    "# nomFeatureColNames.remove('rec_ID')\n",
    "\n",
    "print(\"Columns with dtype 'object':\", nomFeatureColNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5844be3-9343-48c4-9b8a-8eef3aa48ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding nominal features...,\n",
    "nominal_categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder(sparse_output=False)),\n",
    "        # ('dbg', Debug()),\n",
    "        # (\"selector\", SelectPercentile(chi2, percentile=50)),\n",
    "    ],\n",
    "    verbose=Verbosity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84188237-2aba-4ca0-967d-c3feb58296fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming clinical_info is a pandas DataFrame already loaded\n",
    "\n",
    "# Select columns where dtype is 'object'\n",
    "numeric_columns = fianl_clean_data.select_dtypes(exclude=['object']).columns\n",
    "listOfNumericalColNames=[]\n",
    "# Convert the Index object to a list\n",
    "listOfNumericalColNames = numeric_columns.tolist()\n",
    "\n",
    "# Print the list of column names\n",
    "\n",
    "# listOfNumericalColNames.remove('PFI.time')\n",
    "print(\"Columns with dtype 'object':\", listOfNumericalColNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4a0bd35-6dce-44aa-af38-32cd6b1ae4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Verbosity =False\n",
    "# print(listOfNumericalColNames.count('T'))\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler()),\n",
    "    # ('dbg', Debug())\n",
    "    ],\n",
    "    verbose=Verbosity\n",
    ")\n",
    "# mergeSurvivalClinicalGene.count('T')\n",
    "# listOfNumericalColNames.remove('DFI')\n",
    "numericColsWithNans = pd.DataFrame(fianl_clean_data[listOfNumericalColNames].isna().sum()[lambda x : x > 0])\n",
    "\n",
    "numericColsWithNans.set_index(numericColsWithNans.index.astype(str), inplace=True)\n",
    "# numericColsWithNans.set_index('num__' + numericColsWithNans.index.astype(str), inplace=True)\n",
    "if Verbosity > 0:\n",
    "    print('\\nnumericColsWithNans: ', numericColsWithNans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae48f854-e4ee-448e-903d-2fb5580ef35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility pipeline class for removing columns containing any NaN\n",
    "# to prepare data for models that cannot handle missing data\n",
    "class RemoveColsWithNan(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X):\n",
    "        if Verbosity > 0:\n",
    "            print('X.shape: ', X.shape)\n",
    "            # print('X type: ', type(X))\n",
    "        X.drop(columns=numericColsWithNans.index, inplace=True)\n",
    "        if Verbosity > 0:\n",
    "            print('\\nNaNCols\\n', numericColsWithNans.index)\n",
    "            print('\\nDropped ', numericColsWithNans.shape[0], ' columns\\n')\n",
    "            print('X.shape: ', X.shape)\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4f9b7ac-495c-420b-9b88-e0c3d78168a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pre-processor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"passthru\", 'passthrough', listOfNumericalColNames),\n",
    "        # (\"cat_ord\", ordinal_categorical_transformer, ordFeatureColNames),\n",
    "        # ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), [3]),\n",
    "        (\"cat_nom\", nominal_categorical_transformer, nomFeatureColNames)\n",
    "        # remainder='passthrough'\n",
    "        # (\"pass_through\", 'passthrough', listOfReadyColNames),\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    "    verbose=Verbosity,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2eade4a-c11f-45c8-8675-b24449723d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pre-processor\n",
    "# def all_cols_selector(X):\n",
    "#     print(list(X.columns))\n",
    "#     return list(X.columns)\n",
    "\n",
    "scaler = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"scaler\", numeric_transformer, make_column_selector()),\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    "    verbose=Verbosity,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8851afd0-a460-449d-9b37-3602732bd6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate feature importances for Cox PH algorithm\n",
    "def cox_fit_and_score_features(X, y):\n",
    "    n_features = X.shape[1]\n",
    "    last_y = y\n",
    "    if Verbosity > 0:\n",
    "        print('n_features: ', n_features)\n",
    "    coxScores = np.empty(n_features)\n",
    "    m = CoxPHSurvivalAnalysis(alpha=0.1, verbose=0)\n",
    "    for j in range(n_features):\n",
    "        Xj = X[:, j : j + 1]\n",
    "        m.fit(Xj, y)\n",
    "        if Verbosity > 0:\n",
    "            print(j, end=',')\n",
    "        coxScores[j] = m.score(Xj, y)\n",
    "\n",
    "    with open('coxScores.pkl', 'wb') as f:\n",
    "        pickle.dump(coxScores, f)\n",
    "\n",
    "    if Verbosity > 0:\n",
    "        print('\\nFinished fitting and scoring\\n')\n",
    "    return coxScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c9cfb6a-32e8-44ca-afb1-e2b0610ad985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model evaluation results across target time horizons\n",
    "def plot_results(models_results, y_timeframes, verbosity):\n",
    "\n",
    "    x = np.arange(len(models_results['XGB']['timeframes'].keys()))  # the label locations\n",
    "\n",
    "    cIndex_vals = {}\n",
    "    for model in models_results.keys():\n",
    "        cIndex_vals[model] = []\n",
    "\n",
    "    for target in models_results['XGB']['timeframes'].keys():\n",
    "        if verbosity > 0:\n",
    "            print(target)\n",
    "            # print(models_results[model]['timeframes'].keys())\n",
    "\n",
    "        for model in models_results.keys():\n",
    "            if verbosity > 0:\n",
    "                print(model)\n",
    "                print(models_results[model]['timeframes'][target]['cIndex_test'])\n",
    "            cIndex_vals[model].append(models_results[model]['timeframes'][target]['cIndex_test'])\n",
    "\n",
    "    print(cIndex_vals)\n",
    "\n",
    "    for model, measurement in cIndex_vals.items():\n",
    "        plt.plot(x, measurement, label=model)\n",
    "\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    plt.ylabel(\"concordance index\")\n",
    "    plt.xlabel(\"analysis target timeframe\")\n",
    "    plt.xticks(x, models_results['XGB']['timeframes'].keys())\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title('Survival Prediction Accuracy by Time Horizon')\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a19cfe8-d785-4137-98d1-c8ea4caee5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "489298f1-55d2-4b5f-843b-8ae1f0307b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Float64Index: 462 entries, 764.0 to 147.0\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   PFI       462 non-null    float64\n",
      " 1   PFI_time  462 non-null    float64\n",
      " 2   PFIBool   462 non-null    bool   \n",
      "dtypes: bool(1), float64(2)\n",
      "memory usage: 27.4 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Float64Index: 462 entries, 764.0 to 147.0\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   PFI       462 non-null    float64\n",
      " 1   PFI_time  462 non-null    float64\n",
      " 2   PFIBool   462 non-null    bool   \n",
      "dtypes: bool(1), float64(2)\n",
      "memory usage: 27.4 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# yBool_length = len(y_train.PFIBool)\n",
    "# yPFITime_length = len(X_train.PFI_Time_nature2012)\n",
    "y_train.info()\n",
    "# Assuming y_train is a pandas DataFrame already loaded\n",
    "\n",
    "# Rename the 'PFI.time' column to 'PFI_time'\n",
    "y_train = y_train.rename(columns={'PFI.time': 'PFI_time'})\n",
    "\n",
    "# Optionally, check the change by printing the DataFrame info to confirm the rename\n",
    "print(y_train.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "803c2222-04e5-4d27-8c43-106ec2978246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of yBool: 462\n",
      "Length of yPFITime: 462\n"
     ]
    }
   ],
   "source": [
    "yBool_length = len(y_train.PFIBool)\n",
    "yPFITime_length = len(y_train.PFI_time)\n",
    "\n",
    "print(f\"Length of yBool: {yBool_length}\")\n",
    "print(f\"Length of yPFITime: {yPFITime_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7a184d3d-0fea-43cd-b22b-b86d003dc785",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_indices = [index for index in ytrain.index if index < yBool_length and index < yPFITime_length]\n",
    "ytrain_filtered = ytrain[valid_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a9d8129-a6e7-40d7-8e58-c414c382d0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of ytest.index: 116\n",
      "Length of yBool: 462\n",
      "Length of yDFITime: 462\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of ytest.index:\", len(ytest.index))\n",
    "print(\"Length of yBool:\", yBool_length)\n",
    "print(\"Length of yDFITime:\",yPFITime_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "929146ee-bd53-4e5d-a1f4-4058f3103992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xgboost in c:\\users\\t00758722\\appdata\\roaming\\python\\python39\\site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\t00758722\\appdata\\roaming\\python\\python39\\site-packages (from xgboost) (1.22.4)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.7.3)\n"
     ]
    }
   ],
   "source": [
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b8e4603-af2e-487a-9589-7fafc082ca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pre-processor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"passthru\", 'passthrough', listOfNumericalColNames),\n",
    "        # (\"cat_ord\", ordinal_categorical_transformer, ordFeatureColNames),\n",
    "        # ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), [3]),\n",
    "        (\"cat_nom\", nominal_categorical_transformer, nomFeatureColNames)\n",
    "        # remainder='passthrough'\n",
    "        # (\"pass_through\", 'passthrough', listOfReadyColNames),\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    "    verbose=Verbosity,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29d2da9e-8993-41ec-a78e-34b7d5f24bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f48859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22634608-e035-4bca-978f-23d096556483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fit Model:  LogisticRegression(random_state=42, verbose=False) ---------------------------------------------\n",
      "\n",
      "Target:  Year1 ---------------------------------------------\n",
      "\n",
      "Score Model on Training Set:  LogisticRegression(random_state=42, verbose=False) ---------------------------------------------\n",
      "model score train: 0.842\n",
      "\n",
      "Score Model on Test Set:  LogisticRegression(random_state=42, verbose=False) ---------------------------------------------\n",
      "model score: 0.836\n",
      "cIndex_test:  0.8096699923254029\n",
      "\n",
      "Fit Model:  LogisticRegression(random_state=42, verbose=False) ---------------------------------------------\n",
      "\n",
      "Target:  Year3 ---------------------------------------------\n",
      "\n",
      "Score Model on Training Set:  LogisticRegression(random_state=42, verbose=False) ---------------------------------------------\n",
      "model score train: 0.816\n",
      "\n",
      "Score Model on Test Set:  LogisticRegression(random_state=42, verbose=False) ---------------------------------------------\n",
      "model score: 0.853\n",
      "cIndex_test:  0.7696693272519954\n",
      "\n",
      "Fit Model:  LogisticRegression(random_state=42, verbose=False) ---------------------------------------------\n",
      "\n",
      "Target:  Year5 ---------------------------------------------\n",
      "\n",
      "Score Model on Training Set:  LogisticRegression(random_state=42, verbose=False) ---------------------------------------------\n",
      "model score train: 0.846\n",
      "\n",
      "Score Model on Test Set:  LogisticRegression(random_state=42, verbose=False) ---------------------------------------------\n",
      "model score: 0.793\n",
      "cIndex_test:  0.6876712328767123\n",
      "\n",
      "Fit Model:  LogisticRegression(random_state=42, verbose=False) ---------------------------------------------\n",
      "\n",
      "Target:  Year7 ---------------------------------------------\n",
      "\n",
      "Score Model on Training Set:  LogisticRegression(random_state=42, verbose=False) ---------------------------------------------\n",
      "model score train: 0.835\n",
      "\n",
      "Score Model on Test Set:  LogisticRegression(random_state=42, verbose=False) ---------------------------------------------\n",
      "model score: 0.836\n",
      "cIndex_test:  0.7236997635933806\n",
      "\n",
      "Fit Model:  LogisticRegression(random_state=42, verbose=False) ---------------------------------------------\n",
      "\n",
      "Target:  End Of Study ---------------------------------------------\n",
      "\n",
      "Score Model on Training Set:  LogisticRegression(random_state=42, verbose=False) ---------------------------------------------\n",
      "model score train: 0.846\n",
      "\n",
      "Score Model on Test Set:  LogisticRegression(random_state=42, verbose=False) ---------------------------------------------\n",
      "model score: 0.836\n",
      "cIndex_test:  0.7226807057484348\n",
      "\n",
      "Fit Model:  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=True, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
      "              min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...) ---------------------------------------------\n",
      "\n",
      "Target:  Year1 ---------------------------------------------\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "Invalid Parameter format for verbosity expect int but value='false'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 35>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFit Model: \u001b[39m\u001b[38;5;124m'\u001b[39m, classifier, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---------------------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTarget: \u001b[39m\u001b[38;5;124m'\u001b[39m, timeframe_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---------------------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Score the model against train set\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mScore Model on Training Set: \u001b[39m\u001b[38;5;124m'\u001b[39m, classifier, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---------------------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\pipeline.py:469\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    468\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 469\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\pipeline.py:406\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params)\u001b[0m\n\u001b[0;32m    404\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    405\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\memory.py:577\u001b[0m, in \u001b[0;36mMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;66;03m# Return the output, without the metadata\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshelving\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\memory.py:532\u001b[0m, in \u001b[0;36mMemorizedFunc._cached_call\u001b[1;34m(self, args, kwargs, shelving)\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing func \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, argument hash \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min location \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    529\u001b[0m     )\n\u001b[0;32m    531\u001b[0m \u001b[38;5;66;03m# Returns the output but not the metadata\u001b[39;00m\n\u001b[1;32m--> 532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshelving\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\memory.py:771\u001b[0m, in \u001b[0;36mMemorizedFunc._call\u001b[1;34m(self, call_id, args, kwargs, shelving)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_call(args, kwargs)\n\u001b[0;32m    770\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 771\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_call(call_id, args, kwargs, shelving,\n\u001b[0;32m    773\u001b[0m                         output, start_time)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\pipeline.py:1310\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1310\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1312\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1313\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1314\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:1101\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\feature_selection\\_rfe.py:777\u001b[0m, in \u001b[0;36mRFECV.fit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    774\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    775\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[1;32m--> 777\u001b[0m scores_features \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrfe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m scores, step_n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mscores_features)\n\u001b[0;32m    783\u001b[0m step_n_features_rev \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(step_n_features[\u001b[38;5;241m0\u001b[39m])[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\feature_selection\\_rfe.py:778\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    774\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    775\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[0;32m    777\u001b[0m scores_features \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m--> 778\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrfe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    780\u001b[0m )\n\u001b[0;32m    781\u001b[0m scores, step_n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mscores_features)\n\u001b[0;32m    783\u001b[0m step_n_features_rev \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(step_n_features[\u001b[38;5;241m0\u001b[39m])[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\feature_selection\\_rfe.py:37\u001b[0m, in \u001b[0;36m_rfe_single_fit\u001b[1;34m(rfe, estimator, X, y, train, test, scorer)\u001b[0m\n\u001b[0;32m     34\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, train)\n\u001b[0;32m     35\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, test, train)\n\u001b[1;32m---> 37\u001b[0m \u001b[43mrfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# TODO(SLEP6): pass score_params here\u001b[39;49;00m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rfe\u001b[38;5;241m.\u001b[39mstep_scores_, rfe\u001b[38;5;241m.\u001b[39mstep_n_features_\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\feature_selection\\_rfe.py:323\u001b[0m, in \u001b[0;36mRFE._fit\u001b[1;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[1;32m--> 323\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X[:, features], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[0;32m    326\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[0;32m    327\u001b[0m     estimator,\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[0;32m    329\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    330\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1466\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1450\u001b[0m \u001b[38;5;129m@_deprecate_positional_args\u001b[39m\n\u001b[0;32m   1451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m   1452\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1464\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGBClassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1465\u001b[0m     \u001b[38;5;66;03m# pylint: disable = attribute-defined-outside-init,too-many-statements\u001b[39;00m\n\u001b[1;32m-> 1466\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity):\n\u001b[0;32m   1467\u001b[0m         evals_result: TrainingCallback\u001b[38;5;241m.\u001b[39mEvalsLog \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1468\u001b[0m         \u001b[38;5;66;03m# We keep the n_classes_ as a simple member instead of loading it from\u001b[39;00m\n\u001b[0;32m   1469\u001b[0m         \u001b[38;5;66;03m# booster in a Python property. This way we can have efficient and\u001b[39;00m\n\u001b[0;32m   1470\u001b[0m         \u001b[38;5;66;03m# thread-safe prediction.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\config.py:181\u001b[0m, in \u001b[0;36mconfig_context\u001b[1;34m(**new_config)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;129m@config_doc\u001b[39m(\n\u001b[0;32m    157\u001b[0m     header\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m )\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfig_context\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_config: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[0;32m    180\u001b[0m     old_config \u001b[38;5;241m=\u001b[39m get_config()\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m--> 181\u001b[0m     set_config(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_config)\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    184\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\config.py:108\u001b[0m, in \u001b[0;36mconfig_doc.<locals>.config_doc_decorator.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\config.py:132\u001b[0m, in \u001b[0;36mset_config\u001b[1;34m(**new_config)\u001b[0m\n\u001b[0;32m    130\u001b[0m         not_none[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[0;32m    131\u001b[0m config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(not_none)\n\u001b[1;32m--> 132\u001b[0m \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBSetGlobalConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:284\u001b[0m, in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[1;31mXGBoostError\u001b[0m: Invalid Parameter format for verbosity expect int but value='false'"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # start the timer\n",
    "classifiers = [\n",
    "    ['Log Reg', LogisticRegression(verbose=Verbosity,\n",
    "                                   random_state=randomState)],\n",
    "#     ['XGB', xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "#                               verbosity=Verbosity,\n",
    "#                               reg_lambda=5.0,\n",
    "#                               random_state=randomState,\n",
    "#                               max_depth=4,\n",
    "#                               min_child_weight=6,\n",
    "#                               subsample=0.8,\n",
    "#                               colsample_bytree=0.8,\n",
    "#                               n_estimators=1000,\n",
    "#                               learning_rate=0.01,\n",
    "#                               enable_categorical=True)],\n",
    "    # ['MLP', MLPClassifier(max_iter=300, verbose=Verbosity, random_state=randomState)],\n",
    "#    ['SVM', SVC(kernel='linear',probability=True, verbose=True, random_state=randomState)],\n",
    "#    ['HistGB', HistGradientBoostingClassifier(verbose=Verbosity)],\n",
    "    # ['SVM', LinearSVC(verbose=True, random_state=randomState)]\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "steps = [\n",
    "#         ('preprocess', preprocessor),\n",
    "        ('scaler', scaler),\n",
    "#         (\"stripNaNCols\", RemoveColsWithNan()),\n",
    "        (\"selectK\", SelectKBest(k=25)),\n",
    "        # (\"dump\", Dump()),\n",
    "        # ('select', fs),\n",
    "]\n",
    "pipeline = Pipeline(steps, memory='/kaggle/working/mInfoPipecache', verbose=Verbosity)\n",
    "\n",
    "for name, classifier in classifiers:\n",
    "    pipeline.steps.append(['selectRFECV', RFECV(estimator=deepcopy(classifier), step=1, verbose=Verbosity)])\n",
    "    pipeline.steps.append(['clf', classifier])\n",
    "    models_results[name] = {'model': classifier}\n",
    "    models_results[name]['pipeline'] = deepcopy(pipeline)\n",
    "    models_results[name]['timeframes'] = {}\n",
    "\n",
    "    for timeframe_name, Xtrain, Xtest, ytrain, ytest, y, yBool, yPFITime in y_timeframes:\n",
    "        models_results[name]['timeframes'][timeframe_name] = {}\n",
    "        tf = models_results[name]['timeframes'][timeframe_name]\n",
    "\n",
    "#         cv_refcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=randomState)\n",
    "#         n_scores = cross_val_score(pipeline, mergeSurvivalClinicalGene, y, scoring='accuracy', cv=cv_refcv, n_jobs=-1)\n",
    "#         print('Accuracy: %.2f (%.2f)'% (np.mean(n_scores), np.std(n_scores)))\n",
    "\n",
    "        print('\\nFit Model: ', classifier, '---------------------------------------------')\n",
    "        print('\\nTarget: ', timeframe_name, '---------------------------------------------')\n",
    "        pipeline.fit(Xtrain, ytrain)\n",
    "\n",
    "        # Score the model against train set\n",
    "        print('\\nScore Model on Training Set: ', classifier, '---------------------------------------------')\n",
    "        tf['train_score'] = pipeline.score(Xtrain, ytrain)\n",
    "        print(\"model score train: %.3f\" % tf['train_score'])\n",
    "\n",
    "        # predict against train set\n",
    "        yHat_train = pipeline.predict(Xtrain)\n",
    "        tf['yHat_train'] = yHat_train\n",
    "\n",
    "        # predict probabilities, not just binary\n",
    "        yHat_train_prob = pipeline.predict_proba(Xtrain)\n",
    "        tf['yHat_train_prob'] = yHat_train_prob\n",
    "\n",
    "        # Score the model against Test Set\n",
    "        print('\\nScore Model on Test Set: ', classifier, '---------------------------------------------')\n",
    "        tf['test_score'] = pipeline.score(Xtest, ytest)\n",
    "        print(\"model score: %.3f\" % tf['test_score'])\n",
    "\n",
    "        # predict against test set\n",
    "        yHat_test = pipeline.predict(Xtest)\n",
    "        tf['yHat_test'] = yHat_test\n",
    "        tf['test_f1_score'] = f1_score(ytest, yHat_test)\n",
    "\n",
    "        # predict probabilities, not just binary\n",
    "        yHat_test_prob = pipeline.predict_proba(Xtest)\n",
    "        tf['yHat_test_prob'] = yHat_test_prob\n",
    "\n",
    "        # Score model performance as Concordance Index against Test Set\n",
    "        # Safe handling of indices when accessing yBool and yDFITime\n",
    "        safe_yBool = yBool.loc[ytest.index.intersection(yBool.index)]\n",
    "        safe_yPFITime = yPFITime.loc[ytest.index.intersection(yPFITime.index)]\n",
    "\n",
    "        try:\n",
    "            tf['cIndex_test'] = concordance_index_censored(\n",
    "                safe_yBool,\n",
    "                safe_yPFITime,\n",
    "                yHat_test_prob[:, 1])[0]\n",
    "            print('cIndex_test: ', tf['cIndex_test'])\n",
    "        except Exception as e:\n",
    "            print(\"Error calculating cIndex_test:\", e)\n",
    "\n",
    "        tf['pipeline'] = deepcopy(pipeline)\n",
    "        tf['model'] = deepcopy(classifier)\n",
    "\n",
    "    pipeline.steps.pop(-1)\n",
    "    pipeline.steps.pop(-1)\n",
    "\n",
    "print('%s seconds' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270009d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a349b403-72af-4cb5-bc8b-eca48f559810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fit Model:  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=True, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
      "              min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...) ---------------------------------------------\n",
      "\n",
      "Target:  Year1 ---------------------------------------------\n",
      "\n",
      "Score Model on Training Set:  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=True, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
      "              min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...) ---------------------------------------------\n",
      "model score train: 0.922\n",
      "\n",
      "Score Model on Test Set:  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=True, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
      "              min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...) ---------------------------------------------\n",
      "model score: 0.853\n",
      "cIndex_test:  0.781657712970069\n",
      "\n",
      "Fit Model:  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=True, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
      "              min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...) ---------------------------------------------\n",
      "\n",
      "Target:  Year3 ---------------------------------------------\n",
      "\n",
      "Score Model on Training Set:  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=True, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
      "              min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...) ---------------------------------------------\n",
      "model score train: 0.816\n",
      "\n",
      "Score Model on Test Set:  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=True, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
      "              min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...) ---------------------------------------------\n",
      "model score: 0.853\n",
      "cIndex_test:  0.7696693272519954\n",
      "\n",
      "Fit Model:  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=True, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
      "              min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...) ---------------------------------------------\n",
      "\n",
      "Target:  Year5 ---------------------------------------------\n",
      "\n",
      "Score Model on Training Set:  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=True, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
      "              min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...) ---------------------------------------------\n",
      "model score train: 0.911\n",
      "\n",
      "Score Model on Test Set:  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=True, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
      "              min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...) ---------------------------------------------\n",
      "model score: 0.784\n",
      "cIndex_test:  0.7552511415525114\n",
      "\n",
      "Fit Model:  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=True, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
      "              min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...) ---------------------------------------------\n",
      "\n",
      "Target:  Year7 ---------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score Model on Training Set:  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=True, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
      "              min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...) ---------------------------------------------\n",
      "model score train: 0.835\n",
      "\n",
      "Score Model on Test Set:  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=True, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
      "              min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...) ---------------------------------------------\n",
      "model score: 0.836\n",
      "cIndex_test:  0.7236997635933806\n",
      "\n",
      "Fit Model:  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=True, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
      "              min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...) ---------------------------------------------\n",
      "\n",
      "Target:  End Of Study ---------------------------------------------\n",
      "\n",
      "Score Model on Training Set:  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=True, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
      "              min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...) ---------------------------------------------\n",
      "model score train: 0.846\n",
      "\n",
      "Score Model on Test Set:  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=True, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
      "              min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1000, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...) ---------------------------------------------\n",
      "model score: 0.836\n",
      "cIndex_test:  0.7226807057484348\n",
      "315.6549916267395 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # start the timer\n",
    "classifiers = [\n",
    "#     ['Log Reg', LogisticRegression(verbose=Verbosity,\n",
    "#                                    random_state=randomState)],\n",
    "    ['XGB', xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                              verbosity=0,\n",
    "                              reg_lambda=5.0,\n",
    "                              random_state=randomState,\n",
    "                              max_depth=4,\n",
    "                              min_child_weight=6,\n",
    "                              subsample=0.8,\n",
    "                              colsample_bytree=0.8,\n",
    "                              n_estimators=1000,\n",
    "                              learning_rate=0.01,\n",
    "                              enable_categorical=True)],\n",
    "    # ['MLP', MLPClassifier(max_iter=300, verbose=Verbosity, random_state=randomState)],\n",
    "#    ['SVM', SVC(kernel='linear',probability=True, verbose=True, random_state=randomState)],\n",
    "#    ['HistGB', HistGradientBoostingClassifier(verbose=Verbosity)],\n",
    "    # ['SVM', LinearSVC(verbose=True, random_state=randomState)]\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "steps = [\n",
    "#         ('preprocess', preprocessor),\n",
    "        ('scaler', scaler),\n",
    "#         (\"stripNaNCols\", RemoveColsWithNan()),\n",
    "        (\"selectK\", SelectKBest(k=25)),\n",
    "        # (\"dump\", Dump()),\n",
    "        # ('select', fs),\n",
    "]\n",
    "pipeline = Pipeline(steps, memory='/kaggle/working/mInfoPipecache', verbose=Verbosity)\n",
    "\n",
    "for name, classifier in classifiers:\n",
    "    pipeline.steps.append(['selectRFECV', RFECV(estimator=deepcopy(classifier), step=1, verbose=Verbosity)])\n",
    "    pipeline.steps.append(['clf', classifier])\n",
    "    models_results[name] = {'model': classifier}\n",
    "    models_results[name]['pipeline'] = deepcopy(pipeline)\n",
    "    models_results[name]['timeframes'] = {}\n",
    "\n",
    "    for timeframe_name, Xtrain, Xtest, ytrain, ytest, y, yBool, yPFITime in y_timeframes:\n",
    "        models_results[name]['timeframes'][timeframe_name] = {}\n",
    "        tf = models_results[name]['timeframes'][timeframe_name]\n",
    "\n",
    "#         cv_refcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=randomState)\n",
    "#         n_scores = cross_val_score(pipeline, mergeSurvivalClinicalGene, y, scoring='accuracy', cv=cv_refcv, n_jobs=-1)\n",
    "#         print('Accuracy: %.2f (%.2f)'% (np.mean(n_scores), np.std(n_scores)))\n",
    "\n",
    "        print('\\nFit Model: ', classifier, '---------------------------------------------')\n",
    "        print('\\nTarget: ', timeframe_name, '---------------------------------------------')\n",
    "        pipeline.fit(Xtrain, ytrain)\n",
    "\n",
    "        # Score the model against train set\n",
    "        print('\\nScore Model on Training Set: ', classifier, '---------------------------------------------')\n",
    "        tf['train_score'] = pipeline.score(Xtrain, ytrain)\n",
    "        print(\"model score train: %.3f\" % tf['train_score'])\n",
    "\n",
    "        # predict against train set\n",
    "        yHat_train = pipeline.predict(Xtrain)\n",
    "        tf['yHat_train'] = yHat_train\n",
    "\n",
    "        # predict probabilities, not just binary\n",
    "        yHat_train_prob = pipeline.predict_proba(Xtrain)\n",
    "        tf['yHat_train_prob'] = yHat_train_prob\n",
    "\n",
    "        # Score the model against Test Set\n",
    "        print('\\nScore Model on Test Set: ', classifier, '---------------------------------------------')\n",
    "        tf['test_score'] = pipeline.score(Xtest, ytest)\n",
    "        print(\"model score: %.3f\" % tf['test_score'])\n",
    "\n",
    "        # predict against test set\n",
    "        yHat_test = pipeline.predict(Xtest)\n",
    "        tf['yHat_test'] = yHat_test\n",
    "        tf['test_f1_score'] = f1_score(ytest, yHat_test)\n",
    "\n",
    "        # predict probabilities, not just binary\n",
    "        yHat_test_prob = pipeline.predict_proba(Xtest)\n",
    "        tf['yHat_test_prob'] = yHat_test_prob\n",
    "\n",
    "        # Score model performance as Concordance Index against Test Set\n",
    "        # Safe handling of indices when accessing yBool and yDFITime\n",
    "        safe_yBool = yBool.loc[ytest.index.intersection(yBool.index)]\n",
    "        safe_yPFITime = yPFITime.loc[ytest.index.intersection(yPFITime.index)]\n",
    "\n",
    "        try:\n",
    "            tf['cIndex_test'] = concordance_index_censored(\n",
    "                safe_yBool,\n",
    "                safe_yPFITime,\n",
    "                yHat_test_prob[:, 1])[0]\n",
    "            print('cIndex_test: ', tf['cIndex_test'])\n",
    "        except Exception as e:\n",
    "            print(\"Error calculating cIndex_test:\", e)\n",
    "\n",
    "        tf['pipeline'] = deepcopy(pipeline)\n",
    "        tf['model'] = deepcopy(classifier)\n",
    "\n",
    "    pipeline.steps.pop(-1)\n",
    "    pipeline.steps.pop(-1)\n",
    "\n",
    "print('%s seconds' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d191d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fit Model:  SVC(kernel='linear', probability=True, random_state=42, verbose=True) ---------------------------------------------\n",
      "\n",
      "Target:  Year1 ---------------------------------------------\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]\n",
      "Score Model on Training Set:  SVC(kernel='linear', probability=True, random_state=42, verbose=True) ---------------------------------------------\n",
      "model score train: 0.814\n",
      "\n",
      "Score Model on Test Set:  SVC(kernel='linear', probability=True, random_state=42, verbose=True) ---------------------------------------------\n",
      "model score: 0.836\n",
      "cIndex_test:  0.8058326937835764\n",
      "\n",
      "Fit Model:  SVC(kernel='linear', probability=True, random_state=42, verbose=True) ---------------------------------------------\n",
      "\n",
      "Target:  Year3 ---------------------------------------------\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]\n",
      "Score Model on Training Set:  SVC(kernel='linear', probability=True, random_state=42, verbose=True) ---------------------------------------------\n",
      "model score train: 0.816\n",
      "\n",
      "Score Model on Test Set:  SVC(kernel='linear', probability=True, random_state=42, verbose=True) ---------------------------------------------\n",
      "model score: 0.853\n",
      "cIndex_test:  0.7696693272519954\n",
      "\n",
      "Fit Model:  SVC(kernel='linear', probability=True, random_state=42, verbose=True) ---------------------------------------------\n",
      "\n",
      "Target:  Year5 ---------------------------------------------\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]\n",
      "Score Model on Training Set:  SVC(kernel='linear', probability=True, random_state=42, verbose=True) ---------------------------------------------\n",
      "model score train: 0.846\n",
      "\n",
      "Score Model on Test Set:  SVC(kernel='linear', probability=True, random_state=42, verbose=True) ---------------------------------------------\n",
      "model score: 0.793\n",
      "cIndex_test:  0.6876712328767123\n",
      "\n",
      "Fit Model:  SVC(kernel='linear', probability=True, random_state=42, verbose=True) ---------------------------------------------\n",
      "\n",
      "Target:  Year7 ---------------------------------------------\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]\n",
      "Score Model on Training Set:  SVC(kernel='linear', probability=True, random_state=42, verbose=True) ---------------------------------------------\n",
      "model score train: 0.835\n",
      "\n",
      "Score Model on Test Set:  SVC(kernel='linear', probability=True, random_state=42, verbose=True) ---------------------------------------------\n",
      "model score: 0.836\n",
      "cIndex_test:  0.7236997635933806\n",
      "\n",
      "Fit Model:  SVC(kernel='linear', probability=True, random_state=42, verbose=True) ---------------------------------------------\n",
      "\n",
      "Target:  End Of Study ---------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]\n",
      "Score Model on Training Set:  SVC(kernel='linear', probability=True, random_state=42, verbose=True) ---------------------------------------------\n",
      "model score train: 0.846\n",
      "\n",
      "Score Model on Test Set:  SVC(kernel='linear', probability=True, random_state=42, verbose=True) ---------------------------------------------\n",
      "model score: 0.836\n",
      "cIndex_test:  0.7226807057484348\n",
      "137.87810349464417 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # start the timer\n",
    "classifiers = [\n",
    "#     ['Log Reg', LogisticRegression(verbose=Verbosity,\n",
    "#                                    random_state=randomState)],\n",
    "#     ['XGB', xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "#                               verbosity=0,\n",
    "#                               reg_lambda=5.0,\n",
    "#                               random_state=randomState,\n",
    "#                               max_depth=4,\n",
    "#                               min_child_weight=6,\n",
    "#                               subsample=0.8,\n",
    "#                               colsample_bytree=0.8,\n",
    "#                               n_estimators=1000,\n",
    "#                               learning_rate=0.01,\n",
    "#                               enable_categorical=True)],\n",
    "#     ['MLP', MLPClassifier(max_iter=300, verbose=Verbosity, random_state=randomState)],\n",
    "   ['SVM', SVC(kernel='linear',probability=True, verbose=True, random_state=randomState)],\n",
    "#    ['HistGB', HistGradientBoostingClassifier(verbose=Verbosity)],\n",
    "    # ['SVM', LinearSVC(verbose=True, random_state=randomState)]\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "steps = [\n",
    "#         ('preprocess', preprocessor),\n",
    "        ('scaler', scaler),\n",
    "#         (\"stripNaNCols\", RemoveColsWithNan()),\n",
    "        (\"selectK\", SelectKBest(k=25)),\n",
    "        # (\"dump\", Dump()),\n",
    "        # ('select', fs),\n",
    "]\n",
    "pipeline = Pipeline(steps, memory='/kaggle/working/mInfoPipecache', verbose=Verbosity)\n",
    "\n",
    "for name, classifier in classifiers:\n",
    "    pipeline.steps.append(['selectRFECV', RFECV(estimator=deepcopy(classifier), step=1, verbose=Verbosity)])\n",
    "    pipeline.steps.append(['clf', classifier])\n",
    "    models_results[name] = {'model': classifier}\n",
    "    models_results[name]['pipeline'] = deepcopy(pipeline)\n",
    "    models_results[name]['timeframes'] = {}\n",
    "\n",
    "    for timeframe_name, Xtrain, Xtest, ytrain, ytest, y, yBool, yPFITime in y_timeframes:\n",
    "        models_results[name]['timeframes'][timeframe_name] = {}\n",
    "        tf = models_results[name]['timeframes'][timeframe_name]\n",
    "\n",
    "#         cv_refcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=randomState)\n",
    "#         n_scores = cross_val_score(pipeline, mergeSurvivalClinicalGene, y, scoring='accuracy', cv=cv_refcv, n_jobs=-1)\n",
    "#         print('Accuracy: %.2f (%.2f)'% (np.mean(n_scores), np.std(n_scores)))\n",
    "\n",
    "        print('\\nFit Model: ', classifier, '---------------------------------------------')\n",
    "        print('\\nTarget: ', timeframe_name, '---------------------------------------------')\n",
    "        pipeline.fit(Xtrain, ytrain)\n",
    "\n",
    "        # Score the model against train set\n",
    "        print('\\nScore Model on Training Set: ', classifier, '---------------------------------------------')\n",
    "        tf['train_score'] = pipeline.score(Xtrain, ytrain)\n",
    "        print(\"model score train: %.3f\" % tf['train_score'])\n",
    "\n",
    "        # predict against train set\n",
    "        yHat_train = pipeline.predict(Xtrain)\n",
    "        tf['yHat_train'] = yHat_train\n",
    "\n",
    "        # predict probabilities, not just binary\n",
    "        yHat_train_prob = pipeline.predict_proba(Xtrain)\n",
    "        tf['yHat_train_prob'] = yHat_train_prob\n",
    "\n",
    "        # Score the model against Test Set\n",
    "        print('\\nScore Model on Test Set: ', classifier, '---------------------------------------------')\n",
    "        tf['test_score'] = pipeline.score(Xtest, ytest)\n",
    "        print(\"model score: %.3f\" % tf['test_score'])\n",
    "\n",
    "        # predict against test set\n",
    "        yHat_test = pipeline.predict(Xtest)\n",
    "        tf['yHat_test'] = yHat_test\n",
    "        tf['test_f1_score'] = f1_score(ytest, yHat_test)\n",
    "\n",
    "        # predict probabilities, not just binary\n",
    "        yHat_test_prob = pipeline.predict_proba(Xtest)\n",
    "        tf['yHat_test_prob'] = yHat_test_prob\n",
    "\n",
    "        # Score model performance as Concordance Index against Test Set\n",
    "        # Safe handling of indices when accessing yBool and yDFITime\n",
    "        safe_yBool = yBool.loc[ytest.index.intersection(yBool.index)]\n",
    "        safe_yPFITime = yPFITime.loc[ytest.index.intersection(yPFITime.index)]\n",
    "\n",
    "        try:\n",
    "            tf['cIndex_test'] = concordance_index_censored(\n",
    "                safe_yBool,\n",
    "                safe_yPFITime,\n",
    "                yHat_test_prob[:, 1])[0]\n",
    "            print('cIndex_test: ', tf['cIndex_test'])\n",
    "        except Exception as e:\n",
    "            print(\"Error calculating cIndex_test:\", e)\n",
    "\n",
    "        tf['pipeline'] = deepcopy(pipeline)\n",
    "        tf['model'] = deepcopy(classifier)\n",
    "\n",
    "    pipeline.steps.pop(-1)\n",
    "    pipeline.steps.pop(-1)\n",
    "\n",
    "print('%s seconds' % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
